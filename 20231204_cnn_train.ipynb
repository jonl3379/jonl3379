{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOY57zVXu5YT8pE9WAjj1nr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jonl3379/jonl3379/blob/main/20231204_cnn_train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TRAIN MODELS"
      ],
      "metadata": {
        "id": "PC6I7KUrrxs1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xcqMIP7bpHlc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad34cea5-b781-4120-d75b-bb5b001bda85"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PREFIX = 20231204_oxfordflowers102_6xresnet_10ep_\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "FILELOC = /content/drive/MyDrive/edu/results/\n",
            "DATASET = 193 32 32 102\n",
            "Compiling model 0 1 2 3 4 5 \n",
            " 6  models compiled\n",
            "6 models compiled\n"
          ]
        }
      ],
      "source": [
        "### GLOBALS ###\n",
        "\n",
        "TRAIN = True\n",
        "ENSEMBLE = True\n",
        "DATE = \"20231204\"\n",
        "DATASET = \"oxford_flowers102\"\n",
        "# Choose from {\"oxford_flowers102\", \"cifar100\", \"stanford_dogs\"}\n",
        "MODELS = \"6xresnet\"\n",
        "# Choose from {\"6xresnet\", \"6xefficientnet\"}\n",
        "EPOCHS = 10\n",
        "# Choose from {\"10\", \"20\"}\n",
        "PREFIX = DATE +\"_\"+ DATASET.replace('_','') +\"_\"+ MODELS +\"_\"+ str(EPOCHS) +\"ep_\"\n",
        "print(\"PREFIX =\", PREFIX)\n",
        "\n",
        "### Libraries ###\n",
        "\n",
        "import numpy as np\n",
        "import pickle\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "### File locations ###\n",
        "\n",
        "def fileloc():\n",
        "  try:\n",
        "    from google.colab import drive\n",
        "    drive.mount(\"/content/drive\")\n",
        "    print(\"FILELOC =\", FILELOC)\n",
        "    return \"/content/drive/MyDrive/edu/results/\"\n",
        "  except:\n",
        "    print(\"FILELOC =\", FILELOC)\n",
        "    return \"./results/\"\n",
        "\n",
        "FILELOC = fileloc()\n",
        "\n",
        "### Save and load ###\n",
        "\n",
        "def fixname(name):\n",
        "  if name[-4:] == \".pkl\":\n",
        "    return name\n",
        "  else:\n",
        "    return name + \".pkl\"\n",
        "\n",
        "def save(data, filename, with_prefix = True):\n",
        "  if with_prefix:\n",
        "    full_filename = FILELOC+PREFIX+fixname(filename)\n",
        "  else:\n",
        "    full_filename = FILELOC+fixname(filename)\n",
        "  with open(full_filename, 'wb') as outfile:\n",
        "    pickle.dump(data, outfile)\n",
        "\n",
        "def load(filename, with_prefix = True):\n",
        "  if with_prefix:\n",
        "    full_filename = FILELOC+PREFIX+fixname(filename)\n",
        "  else:\n",
        "    full_filename = FILELOC+fixname(filename)\n",
        "  with open(full_filename, 'rb') as infile:\n",
        "    return pickle.load(infile)\n",
        "\n",
        "### Load dataset and normalize ###\n",
        "\n",
        "def load_standard(dataset):\n",
        "  train_dataset      = tfds.load(dataset, as_supervised=True, split=\"train[:80%]\")\n",
        "  validation_dataset = tfds.load(dataset, as_supervised=True, split=\"train[80%:]\")\n",
        "  test_dataset, info = tfds.load(dataset, as_supervised=True, split=\"test\", with_info=True)\n",
        "  label_count = info.features[\"label\"].num_classes\n",
        "  return train_dataset, validation_dataset, test_dataset, label_count\n",
        "\n",
        "def load_oxford_flowers102(dataset):\n",
        "  ### CAUTION: SWAPPED TRAIN AND TEST FOR SIZE REASONS ###\n",
        "  train_dataset      = tfds.load(\"oxford_flowers102\", as_supervised=True, split=\"test\")\n",
        "  validation_dataset = tfds.load(\"oxford_flowers102\", as_supervised=True, split=\"validation\")\n",
        "  test_dataset, info = tfds.load(\"oxford_flowers102\", as_supervised=True, split=\"train\", with_info=True)\n",
        "  label_count = info.features[\"label\"].num_classes\n",
        "  return train_dataset, validation_dataset, test_dataset, label_count\n",
        "\n",
        "def preprocess_image(image, label, resolution=(224,224)):\n",
        "    image = tf.image.resize(image, resolution)\n",
        "    image = tf.image.convert_image_dtype(image, tf.float32)\n",
        "    image = (image - 0.5) / 0.5  # Normalize to the range [-1, 1]\n",
        "    return image, label\n",
        "\n",
        "def load(dataset):\n",
        "  if dataset in [\"cifar100\", \"stanford_dogs\"]:\n",
        "    train_dataset, validation_dataset, test_dataset, label_count = load_standard(dataset)\n",
        "  elif dataset == \"oxford_flowers102\":\n",
        "    train_dataset, validation_dataset, test_dataset, label_count = load_oxford_flowers102(dataset)\n",
        "  test_dataset = test_dataset.map(preprocess_image).batch(32)\n",
        "  train_dataset = train_dataset.map(preprocess_image).batch(32)\n",
        "  validation_dataset = validation_dataset.map(preprocess_image).batch(32)\n",
        "  return train_dataset, validation_dataset, test_dataset, label_count\n",
        "\n",
        "train_dataset, validation_dataset, test_dataset, label_count = load(DATASET)\n",
        "print(\"DATASET =\", len(train_dataset), len(test_dataset), len(validation_dataset), label_count)\n",
        "\n",
        "### Build and compile models ###\n",
        "\n",
        "def modelmaker(modelnames):\n",
        "  models = []\n",
        "  print(\"Compiling model\", end = ' ')\n",
        "  for i, modelname in enumerate(modelnames):\n",
        "    print(i, end = ' ')\n",
        "    exec(\"models.append(tf.keras.applications.\"+modelname+\"(weights=None, classes=label_count))\")\n",
        "    models[-1].compile(optimizer='adam',\n",
        "                       loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "                       metrics=['accuracy'])\n",
        "  return models\n",
        "\n",
        "if MODELS == \"6xresnet\":\n",
        "  modelnames = [\"ResNet152V2\",\"ResNet152V2\",\"ResNet152V2\",\"ResNet152V2\",\"ResNet152V2\",\"ResNet152V2\"]\n",
        "elif MODELS == \"6xefficientnet\":\n",
        "  modelnames = [\"EfficientNetB0\",\"EfficientNetB0\",\"EfficientNetB0\",\"EfficientNetB0\",\"EfficientNetB0\",\"EfficientNetB0\"]\n",
        "models = modelmaker(modelnames)\n",
        "\n",
        "print(len(models), 'models compiled')\n",
        "\n"
      ]
    }
  ]
}